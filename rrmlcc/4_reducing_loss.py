#!/usr/bin/env python3
# coding: utf-8

"""
降低损失

迭代方法
  迭代学习可能会让您想到“Hot and Cold”这种寻找隐藏物品（如顶针）的儿童游戏。
  在我们的游戏中，“隐藏的物品”就是最佳模型。
  刚开始，您会胡乱猜测（“w1 的值为 0。”），等待系统告诉您损失是多少。
  然后，您再尝试另一种猜测（“w1 的值为 0.5。”），看看损失是多少。
  哎呀，这次更接近目标了。
  实际上，如果您以正确方式玩这个游戏，通常会越来越接近目标。
  这个游戏真正棘手的地方在于尽可能高效地找到最佳模型。

收敛
  通常，您可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。
  这时候，我们可以说该模型已收敛。

- 梯度下降法
- 凸形
- 学习速率（步长，一种超参数）
- 步长过小：花费过多
- 步长过大：反复横跳

注意：在实践中，成功的模型训练并不意味着要找到“完美”（或接近完美）的学习速率。
我们的目标是找到一个足够高的学习速率，该速率要能够使梯度下降过程高效收敛，但又不会高到使该过程永远无法收敛。

批量
  在梯度下降法中，批量指的是用于在单次迭代中计算梯度的样本总数。

随机梯度下降法（SGD）
  每次迭代只使用一个样本。

小批量随机梯度下降法（小批量 SGD）
  小批量通常包含 10-1000 个随机选择的样本。

"""
